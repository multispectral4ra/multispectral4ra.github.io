
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="accv, workshop, computer vision, multispectral camera, autonomous driving, sensors, machine learning">

  <link rel="shortcut icon" href="static/img/site/favicon_mira2.ico">

  <title>ACCV2024 Workshop on Multispectral Imaging for Robotics and Automation (MIRA)</title>
  <meta name="description" content="Use of multispectral cameras for robotics and automation, ACCV 2024 Workshop">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="Workshop on Multispectral Imaging for Robotics and Automation (MIRA)"/>
  <meta property="og:url" content="https://multispectral4ra.github.io/"/>
  <meta property="og:description" content="Use of multispectral cameras for robotics and automation, ACCV 2024 Workshop"/>
  <meta property="og:site_name" content="Workshop on Multispectral Imaging for Robotics and Automation (MIRA)"/>
  <meta property="og:image" content="https://multispectral4ra.github.io/static/img/site/cover.png"/>

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="Workshop on Multispectral Imaging for Robotics and Automation (MIRA)"/>
  <meta name="twitter:image" content="https://multispectral4ra.github.io/static/img/site/cover.png">
  <meta name="twitter:url" content="https://multispectral4ra.github.io/"/>
  <meta name="twitter:description" content="Use of multispectral cameras for robotics and automation, ACCV 2024 Workshop"/>

  <!-- CSS  -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
  <link rel="stylesheet" href="static/css/main.css" media="screen,projection">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

  <style>

    .people-pic {
      max-width: 125px;
      max-height: 125px;
      /*width:300px;*/
      /*height:300px;*/
      object-fit: cover;
      border-radius: 50%;
  }
  </style>

</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#title" class="scroll-link">Introduction</a></li>
        <li><a href="#cfp" class="scroll-link">Call for papers</a></li>
        <li><a href="#speakers" class="scroll-link">Invited Speakers</a></li>
        <li><a href="#accepted" class="scroll-link">Accepted Papers</a></li>
        <li><a href="#schedule" class="scroll-link">Schedule</a></li>
        <li><a href="#organizers" class="scroll-link">Organizers</a></li>
        <li><a href="#contact" class="scroll-link">Contact</a></li>
      </ul>
    </div>

  </div>
</div>

<script>
  const scrollLinks = document.querySelectorAll('.scroll-link');
  const headerOffset = 80; // Height of the fixed header or any other offset you need

  scrollLinks.forEach(link => {
    link.addEventListener('click', (e) => {
      e.preventDefault();
      const targetId = e.target.getAttribute('href');
      const targetElement = document.querySelector(targetId);

      const elementPosition = targetElement.getBoundingClientRect().top;
      const offsetPosition = elementPosition + window.pageYOffset - headerOffset;

      window.scrollTo({
        top: offsetPosition,
        behavior: 'smooth'
      });
    });
  });
</script>



    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row" id="title">
  <div class="col-xs-12">
    <center><h1>Workshop on <br>Multispectral Imaging for Robotics and Automation <br>(MIRA)</h1></center>
    <center><h2>co-located with <a style="color: #22296d; font-weight:bold;" href="https://www.accv2024.org/"> ACCV 2024</a> and <a style="color: #5a5ca9; font-weight:bold;" href="https://www.acml-conf.org/2024/">ACML 2024</a></h2></center>
    <center><h3>Hanoi, Vietnam - December 8th, 2024</h3></center>
  </div>
</div>

<hr />

<!--
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Past Workshops <span class="caret"></span></a>
  <ul class="dropdown-menu">
    <li><a href="../ACCV2022/index.html" target="__blank">ACCV 2022</a></li>
  </ul>
</li>
-->

<!-- <br>
  <center>
  <h1 style="color:red"><a href="https://www.youtube.com/">The <b>video recording</b> of this workshop is here!</a></h1>
  </center>
<br> -->

<!-- 
<div class="alert alert-info" role="alert">
  <b>For online participation Join the Zoom Meeting from <a href="http://stl.yonsei.ac.kr/">here</a>.</b>
</div>
-->


<div class="row" id="teaser">  
    <div>  
    <img src="static/img/site/miraw_cover.png" style="width: 100%; height: auto;"/>
  </div>
</div>



<p><br /></p>
<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      Multispectral imaging is revolutionizing the fields of robotics and automation by providing richer information beyond the visible spectrum. Traditional RGB cameras capture only a narrow band of the electromagnetic spectrum, limiting the data available for computer vision systems. Multispectral cameras expand this capability by sensing light across a broader range of wavelengths, including infrared, ultraviolet, and other portions of the spectrum invisible to the human eye.
      <br><br>
      This additional spectral information unlocks powerful new applications in robotics and automation. Multispectral data can be used for enhanced material classification, detecting various objects, identifying chemical signatures, and perceiving environmental factors like moisture and temperature. Additionally, in autonomous driving, multispectral imaging allows vehicles to detect lane markings better, read traffic signals, and identify obstacles in challenging conditions like adverse weather situations and darkness. Such capabilities have transformative potential for industrial inspection, agricultural automation, search and rescue operations, self-driving cars, and countless other domains.
      <br><br>
      The Multispectral Imaging for Robotics and Automation (MIRA) workshop aims to bring together leading researchers exploring this emerging interdisciplinary area at the intersection of multispectral imaging, computer vision, robotics, and automation. 
      <br><br>
      Join us to discuss the latest breakthroughs, share cutting-edge research, and forge new collaborations driving innovation in this exciting field.
    </p>
  </div>
</div>

<p><br /></p>

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call For Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
      <p>
        We invite researchers and practitioners to submit original and unpublished work to the Multispectral Imaging for Robotics and Automation (MIRA) workshop. Relevant topics include but are not limited to:
      </p>
      <ul>
        <li>Multispectral image acquisition and sensor fusion</li>
        <li>Multispectral object detection, tracking, and segmentation</li>
        <li>Industrial inspection with multispectral vision</li>
        <li>Agricultural monitoring and automation</li>
        <li>Non-line-of-sight imaging for autonomous vehicles</li>
        <li>Multispectral perception for adverse weather conditions</li>
        <li>Novel applications of multispectral data in robotics and automation</li>
        <li>Multispectral image reconstruction</li>
        <li>Spectral unmixing and material classification</li>
        <li>Domain adaptation and transfer learning for multispectral data</li>
        <li>Multispectral dataset curation and benchmarking</li>
      </ul>
      <p>
        <u>Paper Submission Guidelines:</u>
        <br> 
        To submit papers for consideration, please utilize the workshop's CMT website: <a style="color:#1a1aff;font-weight:400;" href="https://cmt3.research.microsoft.com/MIRA2024">https://cmt3.research.microsoft.com/MIRA2024.</a>  All submissions should be in PDF format.
        <br> 
        Papers that have been previously published or are currently under review elsewhere will not be accepted. It is imperative that submissions adhere to the formatting standards outlined by the Asian Conference on Computer Vision (ACCV), which can be found at <a style="color:#1a1aff;font-weight:400;" href="https://accv2024.org/author-guidelines/">https://accv2024.org/author-guidelines/.</a>
        <br>
        For consistency, papers must use the ACCV LaTeX template and should not exceed 14 pages, including figures and tables. However, additional pages are permissible solely for references.
        <br>
        You can access the official ACCV LaTeX template files by cloning the template from this <a style="color:#1a1aff;font-weight:400;" href="https://www.overleaf.com/read/xzbzbmyfrjks#04841c">overleaf project</a> or downloading this <a style="color:#1a1aff;font-weight:400;" href="https://drive.google.com/file/d/1zQFrAh4C26MJQaHclV3ljngCAySSq7SH/view?usp=sharing">zip file</a>.
        <br>
        All accepted papers are going to be published in the <a style="color:#1a1aff;font-weight:400;" href="https://openaccess.thecvf.com/ACCV2024_workshops">ACCV 2024 Workshop proceedings</a> and <a style="color:#1a1aff;font-weight:400;" href="https://link.springer.com/conference/accv">Springer ACCV 2024 Workshop LNCS</a>.
        <br>
        <u>Important note: PDF files must be under 20MB.</u>
      </p>
  </div>
</div>


<p><br /></p>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Call for papers announced</td>
          <td>July 9, 2024</td>
        </tr>
        <tr>
          <td>Paper submission deadline</td>
          <td><b><del>September 14, 2024</del></b></td>
        </tr>
        <tr>
          <td>Notifications to accepted papers</td>
          <td><del>September 20, 2024</del></td>
        </tr>
        <tr>
          <td>Paper camera ready</td>
          <td>September 30, 2024</td>
        </tr>
        <tr>
          <td>Workshop date</td>
          <td><b>December 8, 2024</b></td>
        </tr>
      
      </tbody>
    </table>
  </div>
</div>

<p><br /></p>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>

<div class="row">
  <div class="col-md-3">
    <div class="speaker-img-container">
      <a href="https://yangkailun.com/"><img class="people-pic" src="static/img/people/prof_yang.jpg" /></a>
    </div>
  </div>
  <div class="col-md-9">
    <p>
      <b><a href="https://yangkailun.com/">Kailun Yang (Hunan University (HNU))</a></b> Kailun Yang is a Professor at the School of Robotics and the National Engineering Research Center of Robot Visual Perception and Control Technology at Hunan University. He earned his PhD in Information Sensing and Instrumentation from Zhejiang University, where he was jointly supervised by experts from Zhejiang University and the University of Alcalá. Prior to his PhD, he completed dual B.S. degrees in Measurement Technology and Instrumentation from Beijing Institute of Technology and Economics from Peking University. His postdoctoral research at the CV
      Lab, Karlsruhe Institute of Technology, focused on human-computer interaction under the guidance of Prof. Rainer Stiefelhagen. Kailun Yang’s research spans multimodal, high-dimensional, and full-view computational optics and vision, with applications in autonomous driving, blind assistance, intelligent transportation systems, and motion analysis. He has made countless contributions to the field of multispectral imaging, with notable works such as <i>ACNet: Attention Based Network to Exploit Complementary Features for RGBD Semantic Segmentation</i>, <i>CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers</i>, and <i>CFMW: Cross-Modality Fusion Mamba for Multispectral Object Detection under Adverse Weather Conditions</i>.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-3">
    <div class="speaker-img-container">
      <a href="https://www.ruirangerfan.com/"><img class="people-pic" src="static/img/people/proffan.jpeg" /></a>
    </div>
  </div>
  <div class="col-md-9">
    <p>
      <b><a href="https://www.ruirangerfan.com/">Rui Fan (Tongji University)</a></b> Rui (Ranger) Fan is a Full Professor at the College of Electronics & Information Engineering at Tongji University, as well as the Shanghai Research Institute for Intelligent Autonomous Systems (SRIAS), the State Key Laboratory of Intelligent Autonomous Systems, and the Frontiers Science Center for Intelligent Autonomous Systems. He directs the Machine Intelligence & Autonomous Systems (MIAS) Group and serves as the General Chair of the Autonomous Vehicle Vision (AVVision) Community.
      Born in Jining, Inner Mongolia, China, in February 1993, Rui completed his B.Eng. at the Harbin Institute of Technology in 2015, and his PhD at the University of Bristol in 2018, with a thesis titled *Real-Time Computer Stereo Vision for Automotive Applications*. After completing his PhD, Rui held research positions at the Hong Kong University of Science and Technology and the University of California San Diego, where he worked with leading experts in robotics and computer vision. He began his faculty career in 2021 and was promoted to Full Professor in 2022, focusing on the intersection of machine intelligence, autonomous systems, and vision technologies for autonomous vehicles.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-3">
    <div class="speaker-img-container">
      <a href="https://ukcheolshin.github.io/"><img class="people-pic" src="static/img/people/ukcheolshin.jpg" /></a>
    </div>
  </div>
  <div class="col-md-9">
    <p>
      <b><a href="https://ukcheolshin.github.io/">Ukcheol Shin (Carnegie Mellon University)</a></b> Ukcheol Shin is a postdoctoral researcher at the Robotics Institute of Carnegie Mellon University. His research focuses on developing a robust robot vision system that can perceive and navigate the dynamic world, even in challenging conditions, with a particular interest in self-supervised learning of 3D geometry and multi-sensor fusion. Dr. Shin has made significant contributions to the field of multispectral imaging, with a focus on topics such as thermal image segmentation and depth estimation on thermal images. Furthermore, he is one of the authors of the MS2 dataset, a valuable resource for the research community.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-3">
    <div class="speaker-img-container">
      <a href="https://stl.yonsei.ac.kr/"><img class="people-pic" src="static/img/people/yonsei_logo.png" /></a>
    </div>
  </div>
  <div class="col-md-9">
    <p>
      <b><a href="https://sites.google.com/yonsei.ac.kr/ashutoshmishra">Ashutosh Mishra (BITS Pilani)</a></b> Ashutosh Mishra is an academic and researcher specializing in Intelligent Systems. He holds a B.Tech. from Uttar Pradesh Technical University (2008), an M.Tech. from NIT Allahabad (2011), and a Ph.D. from IIT (BHU) Varanasi (2018). He was an Assistant Professor at NIT Raipur and received the Korea Research Fellowship in 2019. From 2019 to 2023, he was a Brain Pool Fellow at Yonsei University, South Korea. Currently, he is an Assistant Professor in the Department of Electrical & Electronics Engineering at BITS Pilani, Dubai. His research interests include smart sensors, intelligent systems, autonomous vehicles, convergence technology, and artificial intelligence.
    </p>
  </div>
</div>


<!-- <p><br /></p>
<div class="row" id="accepted">
  <div class="col-xs-12">
    <h2>Accepted Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-12">
    <table>
      <tbody> 
      <tr><td><a href="https://arxiv.org/abs/2212.01558">#1. PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, Hao Su</font></td></tr>
    </tbody></table>
  </div>

</div> -->


<p><br /></p>
<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule (Hanoi, Vietnam / Indochina Time Zone (ICT) (*GMT +7))</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <thead>
        <tr>
          <th>Time</th>
          <th>Particulars</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>10:00 AM - 10:15 AM</td>
          <td>Welcome</td>
        </tr>
        <tr>
          <td>10:15 AM - 10:45 AM</td>
          <td>Opening Talk by Prof. Shiho Kim, Yonsei University, Seoul, Korea</td>
        </tr>
        <tr>
          <td>10:50 AM - 11:35 AM</td>
          <td>Invited Talk by Dr. Ukcheol Shin, Carnegie Mellon University, Pittsburgh, PA, USA</td>
        </tr>
        <tr>
          <td></td>
          <td><b>Paper Session 1</b></td>
        </tr>
        <tr>
          <td>11:40 AM - 12:00 PM</td>
          <td>Accepted Papers Presentation #1</td>
        </tr>
        <tr>
          <td>12:05 PM - 12:25 PM</td>
          <td>Accepted Papers Presentation #2</td>
        </tr>
        <tr>
          <td>12:30 PM - 01:30 PM</td>
          <td>Coffee Break</td>
        </tr>
        <tr>
          <td>01:30 PM - 02:15 PM</td>
          <td>Invited Talk</td>
        </tr>
        <tr>
          <td></td>
          <td><b>Paper Session 2</b></td>
        </tr>
        <tr>
          <td>02:20 PM - 02:40 PM</td>
          <td>Accepted Papers Presentation #3</td>
        </tr>
        <tr>
          <td>02:45 PM - 03:05 PM</td>
          <td>Accepted Papers Presentation #4</td>
        </tr>
        <tr>
          <td>03:05 PM - 03:10 PM</td>
          <td>Concluding remarks</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<p><br /></p>

<div class="row" id="contact">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      <li><a href="https://ynalcakan.github.io/">Dr. Yağız Nalçakan</a> - Seamless Trans-X Lab - Yonsei University</li>
      <li><a href="https://stl.yonsei.ac.kr/">Yeongwan Jin</a> - Seamless Trans-X Lab - Yonsei University</li>
      <li><a href="https://stl.yonsei.ac.kr/">Hyeongjin Ju</a> - Seamless Trans-X Lab - Yonsei University</li>
      <li><a href="https://stl.yonsei.ac.kr/">Hanbin Song</a> - Seamless Trans-X Lab - Yonsei University</li>
      <li><a href="https://stl.yonsei.ac.kr/">Incheol Park</a> - Seamless Trans-X Lab - Yonsei University</li>
    </p>
  </div>
</div>

<p><br /></p>

<div class="row" id="contact">
  <div class="col-xs-12">
    <h2>Program Committee</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      <li><a href="https://stl.yonsei.ac.kr/theme/s007/index/sub1_2.php">Prof. Shiho Kim (program chair)</a> - Yonsei University</li>
      <li>Prof. Guofa Li - Chongqing University</li>
      <li>Prof. Chih-Hsien Hsia - National Ilan University</li>
      <li>Dr. Ashutosh Mishra - Birla Institute of Technology and Science</li>
      <li>Dr. Jianwu Fang - Chang'an University</li>
      <li>Dr. Jifeng Shen - Jiangsu University</li>
      <li>Dr. Di Yuan - Xidian University</li>
      
    </p>
  </div>
</div>

<p><br /></p>

<div class="row" id="contact">
  <div class="col-xs-12">
    <h2>Contact</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      To contact the organizers please use <a href="mailto:multispectral4ra@outlook.com">multispectral4ra@outlook.com</a>
    </p>
  </div>
</div>
<p><br /></p>

<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> and <span style="color:#1a1aff;font-weight:400;"> <a href="https://languagefor3dscenes.github.io/"> L3DS workshop team</a></span> for the webpage format.
    </p>
  </div>
  <div class="col-xs-3" style="width: 200px; height: 150px; display: flex; align-items: center; justify-content: center;">
    <a href="https://www.yonsei.ac.kr/">
      <img src="static/img/site/yonsei-university-logo.jpg"/>
    </a>
  </div>

  <div class="col-xs-3" style="width: 150px; height: 150px; display: flex; align-items: center; justify-content: center;">
    <a href="https://stl.yonsei.ac.kr/">
      <img src="static/img/site/stl-logo-blue.png"/>
    </a>
  </div>
</div>
<br>
<br>
<br>
      </div>
    </div>

  </body>
</html>
